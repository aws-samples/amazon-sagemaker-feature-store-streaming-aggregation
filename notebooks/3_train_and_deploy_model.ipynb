{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Deploying the Fraud Detection Model\n",
    "\n",
    "In this notebook, we will take the outputs from the Processing Job in the previous step and use it and train and deploy an XGBoost model. Our historic transaction dataset is initially comprised of data like timestamp, card number, and transaction amount and we enriched each transaction with features about that card number's recent history, including:\n",
    "\n",
    "- `num_trans_last_10m`\n",
    "- `num_trans_last_1w`\n",
    "- `avg_amt_last_10m`\n",
    "- `avg_amt_last_1w`\n",
    "\n",
    "Individual card numbers may have radically different spending patterns, so we will want to use normalized ratio features to train our XGBoost model to detect fraud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import boto3\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DIR = './data'\n",
    "BUCKET = sagemaker.Session().default_bucket()\n",
    "PREFIX = 'training'\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "s3_client = boto3.Session().client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the results of the SageMaker Processing Job ran in the previous step into a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5400000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{LOCAL_DIR}/aggregated/processing_output.csv')\n",
    "#df.dropna(inplace=True)\n",
    "df['cc_num'] = df['cc_num'].astype(np.int64)\n",
    "df['fraud_label'] = df['fraud_label'].astype(np.int64)\n",
    "df.head()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split DataFrame into Train & Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artifically generated dataset contains transactions from `2020-01-01` to `2020-06-01`. We will create a training and validation set out of transactions from `2020-01-15` and `2020-05-15`, discarding the first two weeks in order for our aggregated features to have built up sufficient history for cards and leaving the last two weeks as a holdout test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start = '2020-01-15'\n",
    "training_end = '2020-05-15'\n",
    "\n",
    "training_df = df[(df.datetime > training_start) & (df.datetime < training_end)]\n",
    "test_df = df[df.datetime >= training_end]\n",
    "\n",
    "test_df.to_csv(f'{LOCAL_DIR}/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we now have lots of information about each transaction in our training dataset, we don't want to pass everything as features to the XGBoost algorithm for training because some elements are not useful for detecting fraud or creating a performant model:\n",
    "- A transaction ID and timestamp is unique to the transaction and never seen again. \n",
    "- A card number, if included in the feature set at all, should be a categorical variable. But we don't want our model to learn that specific card numbers are associated with fraud as this might lead to our system blocking genuine behaviour. Instead we should only have the model learn to detect shifting patterns in a card's spending history. \n",
    "- Individual card numbers may have radically different spending patterns, so we will want to use normalized ratio features to train our XGBoost model to detect fraud. \n",
    "\n",
    "Given all of the above, we drop all columns except for the normalised ratio features and transaction amount from our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.drop(['tid','datetime','cc_num','num_trans_last_10m', 'avg_amt_last_10m',\n",
    "       'num_trans_last_1w', 'avg_amt_last_1w'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [built-in XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) requires the label to be the first column in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>amount</th>\n",
       "      <th>amt_ratio1</th>\n",
       "      <th>amt_ratio2</th>\n",
       "      <th>count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>548.82</td>\n",
       "      <td>1.738022</td>\n",
       "      <td>1.738022</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>221.65</td>\n",
       "      <td>0.713116</td>\n",
       "      <td>0.713116</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>79.19</td>\n",
       "      <td>0.264964</td>\n",
       "      <td>0.264964</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>38.50</td>\n",
       "      <td>0.135008</td>\n",
       "      <td>0.135008</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0</td>\n",
       "      <td>25.22</td>\n",
       "      <td>0.092663</td>\n",
       "      <td>0.092663</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fraud_label  amount  amt_ratio1  amt_ratio2  count_ratio\n",
       "47            0  548.82    1.738022    1.738022     0.055556\n",
       "48            0  221.65    0.713116    0.713116     0.052632\n",
       "49            0   79.19    0.264964    0.264964     0.055556\n",
       "50            0   38.50    0.135008    0.135008     0.052632\n",
       "51            0   25.22    0.092663    0.092663     0.050000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = training_df[['fraud_label', 'amount', 'amt_ratio1','amt_ratio2','count_ratio']]\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(training_df, test_size=0.3)\n",
    "train.to_csv(f'{LOCAL_DIR}/train.csv', header=False, index=False)\n",
    "val.to_csv(f'{LOCAL_DIR}/val.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/train.csv to s3://sagemaker-us-east-1-355151823911/training/train.csv\n",
      "upload: data/val.csv to s3://sagemaker-us-east-1-355151823911/training/val.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {LOCAL_DIR}/train.csv s3://{BUCKET}/{PREFIX}/\n",
    "!aws s3 cp {LOCAL_DIR}/val.csv s3://{BUCKET}/{PREFIX}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-09 12:10:42 Starting - Starting the training job...\n",
      "2020-12-09 12:10:44 Starting - Launching requested ML instances......\n",
      "2020-12-09 12:11:59 Starting - Preparing the instances for training...\n",
      "2020-12-09 12:12:36 Downloading - Downloading input data...\n",
      "2020-12-09 12:12:57 Training - Downloading the training image......\n",
      "2020-12-09 12:14:12 Training - Training image download completed. Training in progress..\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 3009406 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 1289746 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.00132#011validation-error:0.00137\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.00133#011validation-error:0.00138\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.00130#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.00130#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.00130#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.00130#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.00130#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.00130#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.00129#011validation-error:0.00134\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.00128#011validation-error:0.00134\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.00128#011validation-error:0.00134\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.00128#011validation-error:0.00134\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.00129#011validation-error:0.00134\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.00129#011validation-error:0.00134\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.00127#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.00127#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.00128#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.00128#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.00129#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.00128#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.00128#011validation-error:0.00132\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.00128#011validation-error:0.00132\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.00128#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.00128#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.00128#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.00128#011validation-error:0.00132\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.00128#011validation-error:0.00133\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.00128#011validation-error:0.00132\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.00127#011validation-error:0.00132\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.00127#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.00126#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.00126#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.00126#011validation-error:0.00132\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.00126#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.00126#011validation-error:0.00132\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.00126#011validation-error:0.00132\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.00126#011validation-error:0.00132\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.00126#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.00125#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.00125#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.00125#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.00125#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.00125#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.00125#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.00125#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.00124#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.00124#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.00124#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.00124#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.00124#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.00124#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.00124#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.00124#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.00124#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.00123#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.00123#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.00123#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.00123#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.00123#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.00123#011validation-error:0.00131\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.00123#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.00122#011validation-error:0.00129\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.00122#011validation-error:0.00129\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.00122#011validation-error:0.00129\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.00122#011validation-error:0.00129\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.00122#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.00121#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.00121#011validation-error:0.00130\u001b[0m\n",
      "\n",
      "2020-12-09 12:15:44 Uploading - Uploading generated training model\u001b[34m[88]#011train-error:0.00121#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.00121#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.00121#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.00121#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.00121#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.00121#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.00121#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.00121#011validation-error:0.00129\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.00121#011validation-error:0.00129\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.00121#011validation-error:0.00129\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.00120#011validation-error:0.00129\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.00120#011validation-error:0.00129\u001b[0m\n",
      "\n",
      "2020-12-09 12:15:51 Completed - Training job completed\n",
      "Training seconds: 195\n",
      "Billable seconds: 195\n"
     ]
    }
   ],
   "source": [
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"100\"}\n",
    "\n",
    "output_path = 's3://{}/{}/output'.format(BUCKET, PREFIX)\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", sagemaker.Session().boto_region_name, \"1.2-1\")\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}\".format(BUCKET, PREFIX, 'train.csv'), content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}\".format(BUCKET, PREFIX, 'val.csv'), content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would perform hyperparameter tuning before deployment, but for the purposes of this example will deploy the model that resulted from the Training Job directly to a SageMaker hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.t2.medium',\n",
    "    serializer=sagemaker.serializers.CSVSerializer(), wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name' (str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sagemaker-xgboost-2020-12-09-12-16-26-317'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name=predictor.endpoint_name\n",
    "#Store the endpoint name for later cleanup \n",
    "%store endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check that our endpoint is working, let's call it directly with a record from our test hold-out set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'600.89,0.9483045230254504,0.9483045230254504,0.041666666666666664'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload_df = test_df.drop(['tid','datetime','cc_num','fraud_label','num_trans_last_10m', 'avg_amt_last_10m',\n",
    "       'num_trans_last_1w', 'avg_amt_last_1w'], axis=1)\n",
    "payload = payload_df.head(1).to_csv(index=False, header=False).strip()\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0042048427276313305"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(predictor.predict(payload).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show that the model predicts FRAUD / NOT FRAUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With transaction count ratio of: 0.30, fraud score: 0.940\n"
     ]
    }
   ],
   "source": [
    "count_ratio = 0.30\n",
    "payload = f'1.00,1.0,1.0,{count_ratio:.2f}'\n",
    "is_fraud = float(predictor.predict(payload).decode('utf-8'))\n",
    "print(f'With transaction count ratio of: {count_ratio:.2f}, fraud score: {is_fraud:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With transaction count ratio of: 0.06, fraud score: 0.004\n"
     ]
    }
   ],
   "source": [
    "count_ratio = 0.06\n",
    "payload = f'1.00,1.0,1.0,{count_ratio:.2f}'\n",
    "is_fraud = float(predictor.predict(payload).decode('utf-8'))\n",
    "print(f'With transaction count ratio of: {count_ratio:.2f}, fraud score: {is_fraud:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
